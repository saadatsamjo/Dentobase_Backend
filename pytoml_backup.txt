[project]
name = "dentobase"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "accelerate>=1.12.0",
    "alembic>=1.18.2",
    "argon2-cffi>=25.1.0",
    "asyncpg>=0.31.0",
    "chromadb>=1.4.1",
    "einops>=0.8.2",
    "fastapi[standard]>=0.128.0",
    "grandalf>=0.8",
    "greenlet>=3.3.1",
    "huggingface-hub>=0.36.2",
    "jose>=1.0.0",
    "langchain>=1.2.7",
    "langchain-chroma>=1.1.0",
    "langchain-community>=0.4.1",
    "langchain-google-genai>=4.2.0",
    "langchain-groq>=1.1.1",
    "langchain-huggingface>=1.2.0",
    "langchain-ollama>=1.0.1",
    "langchain-openai>=1.1.7",
    "langchain-text-splitters>=1.1.0",
    "passlib>=1.7.4",
    "pillow>=12.1.0",
    "psycopg2>=2.9.11",
    "pydantic-settings>=2.12.0",
    "pypdf>=6.6.2",
    "python-dotenv>=1.2.1",
    "python-jose>=3.5.0",
    "resend>=2.21.0",
    "sentence-transformers>=5.2.2",
    "sqlalchemy>=2.0.46",
    "timm>=1.0.24",
    "torch>=2.10.0",
    "torchvision>=0.25.0",
    "transformers>=4.57.6",
    "uvicorn[standard]>=0.40.0",
]









# # app/RAGsystem/llm_client.py
# """
# LLM Client with STRUCTURED OUTPUT using Pydantic
# Updated to properly extract page references from retrieved knowledge
# """
# import logging
# import re
# from typing import Any, Dict, List
# from app.RAGsystem.schemas import ClinicalRecommendationOutput

# from config.ragconfig import rag_settings

# logger = logging.getLogger(__name__)





# class LLMClient:
#     """Unified LLM client with structured Pydantic output."""

#     _instance = None
#     _clients = {}

#     def __new__(cls):
#         if cls._instance is None:
#             cls._instance = super().__new__(cls)
#         return cls._instance

#     def _get_structured_client(self):
#         """
#         Get LLM client with structured output.

#         Uses .with_structured_output() for OpenAI/Anthropic
#         Falls back to prompt-based JSON for Ollama
#         """
#         provider = rag_settings.LLM_PROVIDER

#         logger.info(f"ðŸ¤– Initializing {provider} LLM with structured output")

#         if provider == "openai":
#             from langchain_openai import ChatOpenAI

#             base_client = ChatOpenAI(
#                 model=rag_settings.OPENAI_LLM_MODEL, temperature=rag_settings.LLM_TEMPERATURE
#             )

#             # Use native structured output
#             return base_client.with_structured_output(
#                 ClinicalRecommendationOutput, method="json_mode"
#             )

#         elif provider == "claude":
#             from langchain_anthropic import ChatAnthropic

#             base_client = ChatAnthropic(
#                 model=rag_settings.CLAUDE_LLM_MODEL, temperature=rag_settings.LLM_TEMPERATURE
#             )

#             # Use native structured output
#             return base_client.with_structured_output(ClinicalRecommendationOutput)

#         elif provider == "ollama":
#             # Ollama doesn't support with_structured_output reliably
#             # Use PydanticOutputParser instead
#             from langchain_core.output_parsers import PydanticOutputParser
#             from langchain_ollama import ChatOllama

#             base_client = ChatOllama(
#                 model=rag_settings.OLLAMA_LLM_MODEL,
#                 temperature=rag_settings.LLM_TEMPERATURE,
#                 format="json",
#             )

#             parser = PydanticOutputParser(pydantic_object=ClinicalRecommendationOutput)

#             # Return tuple of (client, parser) for Ollama
#             return (base_client, parser)

#         else:
#             raise ValueError(f"Unknown LLM provider: {provider}")

#     def _extract_available_pages(self, retrieved_knowledge: str) -> List[int]:
#         """
#         Extract all page numbers from the retrieved knowledge context.
#         This ensures we only reference pages that were actually retrieved.

#         Args:
#             retrieved_knowledge: The formatted knowledge context

#         Returns:
#             List of unique page numbers available in the retrieved knowledge
#         """
#         available_pages = set()

#         # Match patterns like "Pages [385]" or "Pages 385" or "[1] [Pages 385, 386]"
#         # More flexible pattern to catch various formats
#         page_patterns = [
#             r"Pages?\s*\[([0-9,\s]+)\]",  # Pages [385] or Page [385]
#             r"Pages?\s*([0-9,\s]+)",  # Pages 385 or Pages 385, 386
#             r"\[Pages?\s*([0-9,\s]+)\]",  # [Pages 385] or [Page 385]
#         ]

#         for pattern in page_patterns:
#             matches = re.findall(pattern, retrieved_knowledge, re.IGNORECASE)
#             for match in matches:
#                 # Extract individual page numbers from comma-separated strings
#                 page_nums = re.findall(r"\d+", match)
#                 available_pages.update(int(p) for p in page_nums)

#         sorted_pages = sorted(list(available_pages))
#         logger.info(f"ðŸ“– Available pages from retrieved knowledge: {sorted_pages}")
#         return sorted_pages

#     def generate_clinical_recommendation(
#         self,
#         patient_context: str,
#         image_findings: str,
#         retrieved_knowledge: str,
#         query: str,
#         knowledge_available: bool,
#         available_pages: List[int] = None,  # NEW PARAMETER
#     ) -> Dict[str, Any]:
#         """
#         Generate structured clinical recommendation.

#         Args:
#             patient_context: Patient demographics and history
#             image_findings: Radiograph analysis results
#             retrieved_knowledge: Retrieved guideline chunks
#             query: Clinical question
#             knowledge_available: Whether any knowledge chunks were retrieved
#             available_pages: List of page numbers actually retrieved (optional, will be extracted if not provided)

#         Returns:
#             Dict matching ClinicalRecommendationOutput schema
#         """
#         provider = rag_settings.LLM_PROVIDER
#         client_or_tuple = self._get_structured_client()

#         # Extract available pages if not provided
#         if available_pages is None and knowledge_available:
#             available_pages = self._extract_available_pages(retrieved_knowledge)
#         elif not knowledge_available:
#             available_pages = []

#         # Build context-aware prompt
#         if knowledge_available and available_pages:
#             available_pages_str = ", ".join(map(str, available_pages))
#             guidelines_instruction = f"""You have clinical guidelines available from the following pages: {available_pages_str}
                        
#                         CRITICAL INSTRUCTIONS FOR reference_pages:
#                         1. ONLY cite page numbers from this list: {available_pages_str}
#                         2. DO NOT make up or invent any page numbers
#                         3. Only include pages whose content you actually used in your recommendation
#                         4. If you don't use information from a specific page, don't include it in reference_pages
#                         5. The reference_pages field must be a list of integers (e.g., [385, 386, 401])
#                         6. DO NOT use text format like "Page 350" - only integers like 350"""
#         elif knowledge_available:
#             # Fallback if page extraction failed
#             guidelines_instruction = """You have clinical guidelines available but page numbers could not be extracted.
#                         CRITICAL: In reference_pages, use an empty list [] since specific page numbers are unavailable."""
#         else:
#             guidelines_instruction = """âš ï¸ WARNING: No clinical guidelines were retrieved from the knowledge base.
#                         You must rely on general dental knowledge.
#                         CRITICAL: In reference_pages, use an empty list [] since no guidelines were retrieved.
#                         DO NOT make up page numbers."""

#         # Construct prompt
#         prompt = f"""You are a clinical decision support system for dentistry.

#                     === PATIENT INFORMATION ===
#                     {patient_context}

#                     === RADIOGRAPHIC FINDINGS ===
#                     {image_findings}

#                     === CLINICAL GUIDELINES ===
#                     {retrieved_knowledge}

#                     === CLINICAL QUERY ===
#                     {query}

#                     === INSTRUCTIONS ===
#                     {guidelines_instruction}

#                     Based on the above information, provide a structured clinical recommendation.

#                     REQUIREMENTS:
#                     1. diagnosis: Primary diagnosis as a single descriptive string
#                     2. differential_diagnoses: List of 2-3 alternative diagnoses
#                     3. recommended_management: Complete treatment plan as ONE STRING with numbered steps
#                     Format: "1. First step; 2. Second step; 3. Third step"
#                     4. reference_pages: {"List of integer page numbers that you actually used (e.g., [385, 386])" if knowledge_available else "Empty list []"}

#                     Provide your structured recommendation:"""

#         logger.info(f"âš™ï¸  Generating structured recommendation...")
#         logger.info(f"   Provider: {provider}")
#         logger.info(f"   Knowledge available: {knowledge_available}")
#         logger.info(f"   Available pages: {available_pages}")

#         try:
#             if provider == "ollama":
#                 # Ollama: use PydanticOutputParser
#                 client, parser = client_or_tuple

#                 # Add format instructions to prompt
#                 format_instructions = parser.get_format_instructions()
#                 full_prompt = f"{prompt}\n\n{format_instructions}"

#                 # Invoke
#                 response = client.invoke(full_prompt)

#                 # Parse with Pydantic
#                 parsed = parser.parse(response.content)
#                 result = parsed.dict()

#             else:
#                 # OpenAI/Claude: use with_structured_output
#                 structured_client = client_or_tuple

#                 # Invoke directly - returns Pydantic object
#                 response = structured_client.invoke(prompt)
#                 result = response.dict()

#             # POST-PROCESSING: Validate and filter reference pages
#             if result.get("reference_pages"):
#                 # Filter to only include pages that were actually available
#                 if available_pages:
#                     filtered_pages = [p for p in result["reference_pages"] if p in available_pages]
#                     if len(filtered_pages) != len(result["reference_pages"]):
#                         logger.warning(
#                             f"âš ï¸  Filtered hallucinated pages: {set(result['reference_pages']) - set(filtered_pages)}"
#                         )
#                     result["reference_pages"] = filtered_pages
#                 else:
#                     # No pages available, clear any hallucinated references
#                     logger.warning(
#                         f"âš ï¸  Clearing hallucinated pages (no pages available): {result['reference_pages']}"
#                     )
#                     result["reference_pages"] = []

#             logger.info(f"âœ…Structured output generated successfully")
#             logger.info(f"   Diagnosis: {result['diagnosis'][:100]}...")
#             logger.info(f"   Differential diagnoses: {len(result['differential_diagnoses'])} items")
#             logger.info(f"   Management: {len(result['recommended_management'])} chars")
#             logger.info(f"   Reference pages: {result['reference_pages']}")

#             # Validate output types
#             self._validate_output_structure(result)

#             return result

#         except Exception as e:
#             logger.error(f"âŒ Structured output generation failed: {e}", exc_info=True)

#             # Return safe fallback
#             return {
#                 "diagnosis": f"Error generating recommendation: {str(e)}",
#                 "differential_diagnoses": [],
#                 "recommended_management": "Unable to generate recommendation. Please consult supervising dentist.",
#                 "reference_pages": [],
#             }

#     def _validate_output_structure(self, result: Dict) -> None:
#         """Validate that output matches expected structure."""
#         required_keys = [
#             "diagnosis",
#             "differential_diagnoses",
#             "recommended_management",
#             "reference_pages",
#         ]

#         for key in required_keys:
#             if key not in result:
#                 raise ValueError(f"Missing required key: {key}")

#         # Type checks
#         if not isinstance(result["diagnosis"], str):
#             raise ValueError(f"diagnosis must be string, got {type(result['diagnosis'])}")

#         if not isinstance(result["differential_diagnoses"], list):
#             raise ValueError(
#                 f"differential_diagnoses must be list, got {type(result['differential_diagnoses'])}"
#             )

#         if not isinstance(result["recommended_management"], str):
#             raise ValueError(
#                 f"recommended_management must be string, got {type(result['recommended_management'])}"
#             )

#         if not isinstance(result["reference_pages"], list):
#             raise ValueError(f"reference_pages must be list, got {type(result['reference_pages'])}")

#         # Validate all items in reference_pages are integers
#         for page in result["reference_pages"]:
#             if not isinstance(page, int):
#                 raise ValueError(f"All items in reference_pages must be integers, got {type(page)}")

#         logger.info(f"âœ…Output structure validated")

#     def get_model_info(self) -> Dict:
#         """Get current LLM configuration."""
#         return {
#             "provider": rag_settings.LLM_PROVIDER,
#             "model": rag_settings.current_llm_model,
#             "temperature": rag_settings.LLM_TEMPERATURE,
#             "structured_output": True,
#             "parser": (
#                 "with_structured_output"
#                 if rag_settings.LLM_PROVIDER in ["openai", "claude"]
#                 else "PydanticOutputParser"
#             ),
#         }


# # Global instance
# llm_client = LLMClient()
